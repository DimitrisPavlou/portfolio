# OptimKit

**optimkit** is a lightweight Python toolkit for classical numerical optimization algorithms, with an emphasis on clarity, mathematical correctness, and educational value.
It provides clean NumPy- and SymPy-based implementations of **one-dimensional**, **multivariable**, and **population-based** optimization methods.

The project is designed to closely follow standard optimization theory while remaining practical and easy to extend.

---

## âœ¨ Features

* ğŸ“ **One-dimensional optimization**

  * Bisection
  * Derivative-based bisection
  * Fibonacci search
  * Golden-section search

* ğŸ“ˆ **Multivariable unconstrained optimization**

  * Steepest Descent
  * Newtonâ€™s Method
  * Levenbergâ€“Marquardt
  * Multiple step-size (Î³) selection strategies

* ğŸ§¬ **Metaheuristics / population-based methods**

  * Classical Genetic Algorithm (selection, crossover, mutation)
  * Grey Wolf Optimizer (GWO)

* ğŸ§  **Symbolic â†’ Numeric workflow**

  * Objective functions defined symbolically using **SymPy**
  * Automatically converted to fast numeric functions via **NumPy**

* ğŸ§© Modular, readable, and easy to extend

---

## âš ï¸ Problem Assumptions

At its current stage, **optimkit assumes unconstrained optimization problems**.

* No equality or inequality constraints are supported
* All multivariable methods operate on unconstrained objective functions
* Constraint handling (e.g. penalty methods, projections, Lagrange multipliers) is **not implemented yet**, but the project structure allows for future extensions.
  
Future versions may extend support to constrained optimization.

---

## ğŸ”§ Installation

Clone the repository:

```bash
git clone https://github.com/your-username/optimkit.git
cd optimkit
```

Install the dependencies:

```bash
pip install -r requirements.txt
```

### Requirements

* Python â‰¥ 3.9
* NumPy
* SymPy

---

## ğŸš€ Quick Usage Examples

### 1ï¸âƒ£ One-dimensional optimization

```python
import sympy as sp
from optimkit.function import Function
from optimkit.opt1d import bisection, diff_bisection, fibonacci, golden_sector

# Create a symbolic function
x = sp.Symbol('x')
f_sym = x**2 - 4*x + 4
f = Function(f_sym, "symbolic", 1)

# Use with any method
a1, b1, ops = bisection(f, 0, 5, 1e-5, 1e-6)
a2, b2, ops = diff_bisection(f, 0, 5, 1e-5)
a3, b3, ops = fibonacci(f, 0, 5, 1e-5, 1e-6)
a4, b4, ops = golden_sector(f, 0, 5, 1e-5)

print("Approximate minimum (biscetion):", (a1[-1] + b1[-1]) / 2)
print("Approximate minimum (differential bisection):", (a2[-1] + b2[-1]) / 2)
print("Approximate minimum (fibonacci):", (a3[-1] + b3[-1]) / 2)
print("Approximate minimum (golden_sector):", (a4[-1] + b4[-1]) / 2)
```

---

### 2ï¸âƒ£ Multivariable optimization (Steepest Descent, Newton Method, Levenberg Marquardt Method)

```python
import sympy as sp
from optimkit.function import Function
from optimkit.optNd import steepest_descent, newton_method, levenberg_marquardt

# Create a multivariate function
x, y = sp.symbols('x y')
f_sym = x**2 + y**2 - 2*x - 4*y + 5
f = Function(f_sym, "symbolic", 2)

# Use any method
x_min1, iters, grads, vals = steepest_descent(f, [0, 0], epsilon=1e-6)
x_min2, iters, grads, vals = newton_method(f, [0, 0], epsilon=1e-6)
x_min3, iters, grads, vals = levenberg_marquardt(f, [0, 0], epsilon=1e-6)

print("Minimum found at:", x_min1[-1])
print("Minimum found at:", x_min2[-1])
print("Minimum found at:", x_min3[-1])
```

---

### 3ï¸âƒ£ Step-size (Î³) selection

Supported strategies for multivariable methods:

* **Constant step size**
* **Armijo backtracking rule**
* **Optimal line search** (via line search on ( f(x_k + \gamma d_k) ))

---

## ğŸ“ Project Structure

```text
optimkit/
â”‚
â”œâ”€â”€ opt1d/              # One-dimensional optimization algorithms
â”‚   â”œâ”€â”€ bisection.py
â”‚   â”œâ”€â”€ diff_bisection.py
â”‚   â”œâ”€â”€ fibonacci.py
â”‚   â””â”€â”€ golden_sector.py
â”‚
â”œâ”€â”€ optNd/              # Multivariable optimization algorithms
â”‚   â”œâ”€â”€ steepest_descent.py
â”‚   â”œâ”€â”€ newton.py
â”‚   â”œâ”€â”€ levenberg_marquardt.py
â”‚   â””â”€â”€ helper_utils.py
â”‚
â”œâ”€â”€ genetic_opt/        # Population-based optimization methods
â”‚   â”œâ”€â”€ classicGA/
â”‚   â”‚   â”œâ”€â”€ GA.py
â”‚   â”‚   â”œâ”€â”€ selection.py
â”‚   â”‚   â”œâ”€â”€ crossover.py
â”‚   â”‚   â””â”€â”€ mutation.py
â”‚   â””â”€â”€ GWO/
â”‚       â””â”€â”€ GWO.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Design Philosophy

* Prefer **clarity over excessive abstraction**
* Algorithms closely follow textbook formulations
* Symbolic definitions first, numeric execution second
* Minimal dependencies
* Suitable for:

  * academic projects
  * numerical optimization coursework
  * research prototypes

---

## ğŸ›£ï¸ Roadmap

Planned improvements:

* Constraint handling techniques
* Quasi-Newton methods (BFGS, L-BFGS)
* Additional metaheuristics
* Documentation generation (Sphinx)

---

## ğŸ“œ License

This project is released under the **MIT License**.

You are free to use, modify, and distribute it with attribution.

---

## ğŸ“š Citation (optional)

If you use **optimkit** in academic work, consider citing it as a software artifact.
