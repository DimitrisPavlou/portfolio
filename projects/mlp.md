# ðŸ§  Deep Neural Network from Scratch (NumPy)

This project is a **from-scratch implementation of a Multilayer Perceptron (MLP)** in Python for both **classification** and **regression** tasks.  
It follows an **object-oriented design** and relies only on the **NumPy** library.  

The project is designed to be educational and extensible â€” for example, future additions may include an **RBF layer** or **additional optimizers**.  

---

## ðŸ“‚ Project Structure

---

## ðŸ”¹ Layers (`nn/layers.py`)

Implements a fully connected **Dense layer** with:

- Forward pass  
- Backward pass  
- Parameter storage for optimizers  
- Weight initialization via `utils/weight_init.py`  
  (Gaussian, Xavier, He, etc.)

---

## ðŸ”¹ Activation Functions (`nn/activations.py`)

Implements the most common activation functions:

- **ReLU**  
- **Sigmoid**  
- **Tanh**  
- **Softmax**

Each activation class includes both `forward()` and `backward()`.

---

## ðŸ”¹ Loss Functions (`nn/loss_functions.py`)

Includes:

- **Mean Squared Error (MSE)**
- **Categorical Cross Entropy**
- **Softmax + Cross Entropy (Combined)**  
  â†’ Numerically stable for classification

---

## ðŸ”¹ Optimizers (`optim/optimizers.py`)

Optimization algorithms implemented from scratch:

- **SGD**
- **Adam**
- **RMSProp**
- **LRScheduler(step, cosine, plateau)**

---

## ðŸ”¹ Training Engine (`training/trainer.py`)

Centralized training loop:

- Batch creation  
- Forward & backward passes  
- Parameter updates  
- Validation support  
- Callback system (via `callbacks.py`)  
  - Early stopping  
  - Logging  
  - Custom hooks  

---

## ðŸ”¹ Data Loader (`data/dataloader.py`)

Simple data loading utilities:

- Shuffling  
- Mini-batch generation  
- Dataset preparation helpers  
- PyTorch Style DataLoader 
---

## ðŸ”¹ Utilities (`utils/`)

Contains helper modules:

- `metrics.py` â†’ Accuracy, etc.  
- `weight_init.py` â†’ Weight initialization schemes (He, Xavier)
- Additional general utilities

---

## ðŸ”¹ Example Notebook (`train.ipynb`)

Demonstrates:

- Loading a dataset (e.g., Fashion-MNIST)  
- Defining the model  
- Training with validation  
- Evaluating performance  

Achieves **~90% accuracy** on Fashion-MNIST.

---

## âœ¨ Key Features

- Full **forward + backward** implementation  
- Completely modular architecture  
- Easy to extend with new layers, activations, losses, or optimizers  
- Works for **classification** and **regression**  
- Clean, readable structure ideal for educational purposes

---

## ðŸ“œ License

MIT License

---

